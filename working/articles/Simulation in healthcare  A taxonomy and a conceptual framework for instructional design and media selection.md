# Simulation in healthcare  A taxonomy and a conceptual framework for instructional design and media selection

## Page 1

Medical Teacher
ISSN: 0142-159X (Print) 1466-187X (Online) Journal homepage: www.tandfonline.com/journals/imte20
Simulation in healthcare: A taxonomy and a
conceptual framework for instructional design
and media selection
Gilles Chiniara, Gary Cole, Ken Brisbin, Dan Huﬀman, Betty Cragg, Mike
Lamacchia, Dianne Norman & Canadian Network For Simulation In
Healthcare, Guidelines Working Group
To cite this article: Gilles Chiniara, Gary Cole, Ken Brisbin, Dan Huﬀman, Betty Cragg, Mike
Lamacchia, Dianne Norman & Canadian Network For Simulation In Healthcare, Guidelines
Working Group (2013) Simulation in healthcare: A taxonomy and a conceptual framework
for instructional design and media selection, Medical Teacher, 35:8, e1380-e1395, DOI:
10.3109/0142159X.2012.733451
To link to this article:  https://doi.org/10.3109/0142159X.2012.733451
Published online: 02 Nov 2012.
Submit your article to this journal
Article views: 22675
View related articles
Citing articles: 57 View citing articles
Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=imte20

## Page 2

2013
2013; 35: e1380–e1395
WEB PAPER
Simulation in healthcare: A taxonomy and a
conceptual framework for instructional design
and media selection
GILLES CHINIARA1, GARY COLE2, KEN BRISBIN3, DAN HUFFMAN3, BETTY CRAGG4, MIKE LAMACCHIA5,
DIANNE NORMAN6 & CANADIAN NETWORK FOR SIMULATION IN HEALTHCARE, GUIDELINES WORKING
GROUP
1Universite´ Laval, Canada, 2Royal College of Physicians and Surgeons of Canada, 3Alberta Health Services eSIM North,
Canada, 4University of Ottawa, Canada, 5Shock Trauma Air Rescue Society, Canada, 6McMaster Children’s Hospital,
Canada
Abstract
Background: Simulation in healthcare lacks a dedicated framework and supporting taxonomy for instructional design (ID) to
assist educators in creating appropriate simulation learning experiences.
Aims: This article aims to fill the identified gap. It provides a conceptual framework for ID of healthcare simulation.
Methods: The work is based on published literature and authors’ experience with simulation-based education.
Results: The framework for ID itself presents four progressive levels describing the educational intervention. Medium is the mode
of delivery of instruction. Simulation modality is the broad description of the simulation experience and includes four modalities
(computer-based simulation, simulated patient (SP), simulated clinical immersion, and procedural simulation) in addition to mixed,
hybrid simulations. Instructional method describes the techniques used for learning. Presentation describes the detailed
characteristics of the intervention.
The choice of simulation as a learning medium is based on a matrix of simulation relating acuity (severity) to opportunity
(frequency) of events, with a corresponding zone of simulation. An accompanying chart assists in the selection of appropriate
media and simulation modalities based on learning outcomes.
Conclusion: This framework should help educators incorporate simulation in their ID efforts. It also provides a taxonomy to
streamline future research and ID efforts in simulation.
Background
Simulation in healthcare is increasingly being used for teaching
and training in health care, not only in technical skills and
patient management but also in competencies related to
patient safety and teamwork, a more novel yet increasingly
important use of simulation (Raemer 2004).
Although rapidly growing, there are still significant gaps in
the empirical evidence related to a wide variety of questions
surrounding the methodologies, the value, and the outcomes of
simulation in healthcare (Cook et al. 2011). This evidence gap
becomes a particular challenge when faculty, clinicians, and
educators are asked to integrate simulation into existing
curricula and training programs. Furthermore, in a context in
which simulation is being considered for use in national
licensure examinations, alignment between simulation centers
and programs in curriculum design is desirable. Hence, an
appropriate framework must be made available to purveyors of
simulation-based
training.
The
Canadian
Network
for
Simulation in Healthcare (CNSH) believes that the design of
such a framework must be based on a comprehensive
instructional
framework
that
supports,
as
a
minimum
Practice points
. Simulation in healthcare is not a monolithic concept and
includes vastly different educational experiences.
. An educational activity using healthcare simulation can
be described through a framework consisting of four
levels:
instructional
medium,
simulation
modality,
instructional method, and presentation.
. The ‘‘zone of simulation matrix’’ identifies those events
that are best adapted for simulation learning.
. The
framework
identifies
four
different
simulation
modalities: procedural simulation, computer-based simulation, simulated clinical immersion, and SP, with the
added methodology of hybrid simulations. Each is best
suited for specific competency domains or learning
outcomes.
. ‘‘Presentation’’ describes the low-level details that can
affect the design of an educational activity. It includes
elements related to feedback, fidelity (realism), choice of
simulator type, scenario design, and team composition.
Correspondence: Gilles Chiniara, Centre Apprentiss, Universite´ Laval, 1050, Avenue de la Me´decine, Pavillon Ferdinand-Vandry, local 3770-F,
Que´bec, QC G1V 0A6, Canada. Tel: (418) 656-2131, ext. 3902; fax: (418) 656-7747; email: Gilles.Chiniara@fmed.ulaval.ca
e1380
ISSN 0142–159X print/ISSN 1466–187X online/13/081380–16  2013 Informa UK Ltd.
DOI: 10.3109/0142159X.2012.733451

## Page 3

requirement, an alignment of unidisciplinary and/or interprofessional educational methodologies, definitions, theories, and
best practices that can, in turn, support a more universal
direction for research and research outcomes.
With the field still in its infancy, there has been no formal
theory of instructional design (ID) for simulation in healthcare.
Moreover, while theoretical frameworks that relate educational
methods to desired objectives do exist, none is specifically
designed for simulation in healthcare. There are some frameworks tailored to specific types of simulations, such as
computerized screen-based simulations (Williams 2003), that
are outside the field of healthcare. Most of the existing and
accepted ID theories either do not take the specifics of
simulation into consideration (such is the case with Kern
et al.’s (2009) six-step approach to curriculum development),
or use simulation as one single, very broad category, for
example, in Robert Gagne´’s principles of ID (Gagne´ et al.
2005). It is thus challenging to implement these theories for the
design of simulation curricula in learning institutions. Such
concerns have presumably hampered local efforts to build
appropriate and effective curricula for simulation in healthcare.
A crucial issue when discussing ID is to clearly distinguish
the tools used for learning from the actual educational
modalities. In simulation, more than in any other field, this
has been a persistent problem in the discussions and publications on simulation training and education. The actual tool
that is the patient simulator is often considered an educational
method, even though it might be used in widely different
educational
experiences,
for
example
to
reproduce
an
encounter with a patient (Lee et al. 2003; Hassan & Sloan
2006), to simulate an adverse event in a dynamic environment
(Kobayashi et al. 2006), or to facilitate training in a single
technique (Monti et al. 1998).
In this article, we propose a conceptual framework for the
ID of educational activities using simulation in healthcare. The
framework is linked to learning outcomes and assists educators in selecting characteristics for the best design of simulation
training interventions. It positions simulation within the
breadth of potential uses. This article also provides a model
for selecting appropriate simulation modalities, inspired by
Robert Gagne´’s extensive work on instructional media selection (Reiser & Gagne´ 1983; Gagne´ & Medsker 1996).
A definition of healthcare simulation
Although simulation in healthcare is not a monolithic concept,
the expression ‘‘healthcare simulation’’ seems appropriate to
describe the wide range of simulation experiences. Healthcare
simulation is an instructional medium used for education,
assessment, and research, which includes several modalities
that have in common the reproduction of certain characteristics of clinical reality. Simulation-based educational activities
rely on experiential learning. As a fundamental requirement,
they must allow participants to affect, to different degrees, the
course of the educational experience through verbal or
physical
interaction
with
the
simulated
components
or
patients.
The scope of the framework presented in this article is
restricted by excluding certain simulation modalities that target
healthcare systems rather than healthcare providers (e.g.,
computer simulations of emergency room patient flow for bed
management purposes).
A framework for ID
The framework for ID in healthcare simulation is based in
part on Cook’s model for research on e-learning (Cook 2005).
Its purpose is to provide a solid foundation on which to
build the characteristics of simulation for a given educational
intervention.
This framework describes an educational activity or intervention using four levels of ID (Figure 1). Each consecutive
level encompasses a defined set of characteristics that correspond to one level of details of the simulation activity. Stated
otherwise, an educational activity consists of several characteristics grouped into distinct levels based on their specific
impact on the overall quality and design of the intervention.
The choices for any characteristic at a given detail level usually
depend on the choices made at the previous levels. At each
level, the choices made are dependent on the actual learning
needs and goals of the activity.
Level 1: Medium
The most encompassing level, medium constitutes the most
basic choice in the ID process. It describes the principal mode
of delivery of instruction (Cook 2005), which determines all
the educational characteristics of the activity. Examples of
media include textbook learning, lectures, and computerbased training. Within this framework, simulation constitutes
one specific medium. Two of its core characteristics, the
imitation of reality and its interactive nature, distinguish it from
the other delivery media (McGuire 1999).
While simulation methodologies have the potential to
support virtually any learning opportunity, they are an
expensive and often scarce medium. Judiciously selecting
simulation as an appropriate medium for a given educational
activity entails an examination of specific characteristics of
that activity, which can be defined in a ‘‘zone of simulation
matrix.’’
The Zone of Simulation Matrix
The decision to use simulation as an instructional medium
should be based on the analysis of two characteristics of the
specific events, series of events or conditions that are the
desired focus of training: acuity and opportunity. Acuity is
defined as the potential severity of an event or a series of
events
and
their
subsequent
impact
on
the
patient.
Opportunity is defined as the frequency in which a particular
department or individual is actively involved in the management of the event. Alternatively, it also includes the likelihood
of discovering a particular problem. A latent problem that is
not easily discovered in the actual setting is akin to an event
that rarely occurs.
These two characteristics define a matrix that can be
divided into four quadrants: high-acuity low-opportunity
(HALO), high-acuity high-opportunity (HAHO), low-acuity
A framework for simulation in healthcare
e1381

## Page 4

low-opportunity (LALO), and low-acuity high-opportunity
(LAHO) (Figure 2). HALO encompasses clinical situations
that have a high potential to severely impact the patient but are
not common occurrences among the targeted group of
learners. Examples of HALO would be the management of
malignant hyperthermia in the operating room (OR), or a mass
casualty
incident
in the emergency
department. HAHO
includes clinical situations that have a high potential to
severely impact the patient but are common occurrences
among the targeted group. Examples of HAHO include
managing a postpartum hemorrhage in a postpartum unit,
and initiating cardiopulmonary bypass in the OR. LALO
includes clinical situations that have a lower potential to
severely impact the patient but are not a common occurrence
among the targeted group. An example of LALO is manual
disimpaction of fecal mass. Finally, LAHO involves clinical
situations that have a lower potential to severely impact the
patient if not managed appropriately and are common
occurrences among the targeted group. Examples of LAHO
include many of the tasks related to routine patient care and
uncomplicated induction of anesthesia in the OR.
The ‘‘zone of simulation’’ is that area in which simulation
may be advantageous over other instructional media. Within
this zone, simulation can serve as an acceptable substitute or
complement to other, less expensive, media and methods. The
zone of simulation encompasses all HALO situations, and,
when feasible, most HAHO and LALO situations (Figure 2).
While simulation could be used for acquiring introductory
skills in LAHO situations, it is probably not the most efficient
method in that context.
Level 2: Simulation Modality
The second level (called ‘‘configuration’’ in Cook’s original
framework), describes the simulation modality used for
teaching and learning. The choice of modality determines
a broad set of characteristics that dramatically alter the
learning experience. As such, simulation modalities represent
Figure 1.
The levels of ID for an educational experience using healthcare simulation. Each progressive level constitutes the
building blocks for the level directly above it.
Figure 2.
The zone of simulation matrix. The model is based
on two characteristics of clinical situations, acuity (severity)
and opportunity (frequency) that define four areas of varying
dynamics. The zone of simulation identifies those situations
where healthcare simulation may be advantageous over other
instructional media.
G. Chiniara et al.
e1382

## Page 5

the high-level description of the simulation activity. Their
categorization provides clues to their adaptation for specific
learning outcomes (see section ID in simulation and the
outcomes of learning).
This description is based partly on the concept of the
‘‘immersive learning environment’’ that is often associated with
simulation. This concept can range from a focused application
such as the use of a virtual reality simulator, to a broad
spectrum of dynamic environments such as the replication of
an operating theatre or an emergency department. From an
instructional framework perspective, we have defined immersive learning as any situation which is highly interactive and
engages the learner in such a way that disbelief is suspended
and
the
learner
becomes
an
active
participant
in
the
experience.
The shortcoming of the above definition when applied to a
simulation instructional framework is that it does not reflect
that there is a broad spectrum of ‘‘degrees of immersion.’’ In
simulation, we frequently apply an experiential mode to
learning, as defined by Appelman (2004, p. 73): ‘‘Experiential
Modes (EMs) are components of a learning environment that
focus on the learner’s perception while experiencing any
experiential mode, and through a micro analysis bridges the
gap between instructional development and learner cognition.’’ Within simulation, we can extend Appelman’s definition
of modes to identify the various modalities used to deliver
simulation, and classify simulation modalities into four categories with an additional methodology of hybrid (or blended)
simulation (see Figure 3 and Table 1).
Computer-based simulation is a modality that allows the
user to interact with the simulation experience through the
screen-based interface of a computer. It can be used, for
example, to simulate encounters with patients that are
programmed to respond to user input. Simulated Patients
(SPs) are also a simulation modality that can be used to
replicate encounters with real patients (Bokken et al. 2008;
Cleland et al. 2009). They have been used for more than three
decades in medical education (Barrows 1993). With this
modality, elements of history taking, physical examination,
and clinical reasoning can be learned either using an actual
patient or a surrogate playing the role of an actual patient. The
encounter and behavior of the patient are often standardized
(hence the frequent denomination ‘‘standardized patients’’).
However, contrary to simulated clinical immersion, the environment does not affect to a large extent the way the
educational experience unfolds or the occurrence of specific
events. In fact, SPs can be used ‘‘in classrooms and in many
nonclinical areas’’ (Barrows 1993).
In simulated clinical immersion, the learners are exposed
to particular patient problems. Its distinctive characteristic,
however, is that the environment reproduces the actual clinical
or work environment. Interaction with a patient usually takes
place in a fashion similar to SPs, but the environment takes on
an important role in achieving the learning outcomes, or
affects directly the educational experience and the occurrence
of events. For example, the dynamic nature of the environment
and
high
data
flow
of
a
simulated
Emergency
Department can be taken advantage of to teach crisis
management in complex clinical settings; alternatively, an
equipment failure may be the trigger for a specific teaching
case in a simulated operating theatre. In simulated clinical
immersion, the environment can be real (using the actual
clinical setting) or simulated; it can further be small in scale –
reproducing, for example, a single operating room – or can
have a very large scope – a battlefield, a building or even a
city. Importantly, however, the concept of environment
includes not only the physical setting but also the equipment,
teammates, and other individuals involved in reproducing the
desired situation. It also includes elements necessary to the
way
it
is
perceived
and
experienced
(eventually
even
believed) by participants. Thus, simulated clinical immersion
Table 1.
Description of the simulation modalities that constitute
level 2 of the ID framework.
Simulation modality
Description
Computer-based
simulation
Simulation modality in which the user interacts
with the simulation through a screen-based
interface. It can be used for a variety of
learning outcomes.
Hybrid simulation
Simulation experience that combines two or
more simulation modalities.
Procedural
simulation
Simulation modality in which a simulator allows
training of specific psychomotor skills and
their associated procedures.
Simulated clinical
immersion
Simulation modality in which the environment,
real or simulated, plays an important role and
reproduces the actual clinical or work environment. The experience may involve actors,
patients or patient simulators. It is typically
used for training in patient management,
clinical diagnosis, and patient safety
competencies.
Simulated patient
Simulation modality in which an actor, a patient,
or a patient simulator plays the role of an
actual patient (also called standardized
patient). It is typically used for training in
patient management, clinical diagnosis and
affective objectives.
Figure 3.
The four simulation modalities, which are broad
descriptions of the simulation experience. Modalities show
areas of overlap that constitute hybrid simulations.
A framework for simulation in healthcare
e1383

## Page 6

is clearly a social experience (Dieckmann et al. 2007) rather
than the more individual experience that constitutes interaction with SPs.
Finally, procedural simulation focuses on acquiring and
improving procedures and technical skills. Its main characteristic is that it allows the learner to replicate specific behaviors
and movements inherent in the real-life counterpart. It also
allows the learner to train in the specific sequence of actions –
procedures – that are required to appropriately perform a
specific technical skill.
The classification of simulation activities into these four
modalities shows some overlap. However, their nature and the
learning outcomes they each help achieve are different
enough to warrant their consideration as separate modalities.
Nonetheless, in some situations, multiple outcomes can be
achieved by using different simulation modalities at the same
time, which can be described as hybrid (or blended) simulation. Such hybrid simulations allow, for example, training in
technical skills combined with communication proficiency
(Kneebone et al. 2002).
It is important to underline the fact that simulation
modalities and simulator types (see level 4) are distinct
concepts that are unfortunately all too often confounded
(Schiavenato 2009). The same simulator can serve very distinct
purposes. For example, a patient simulator can be used to train
specific technical skills or be used as a patient surrogate in a
scenario of medication error. These are two very different
educational experiences, and the objectives they tackle are
inherently different. Level 2: Simulation Modality aims to
answer the paramount question ‘‘how is simulation being
used?’’ rather than ‘‘what simulator is being used?’’ (a question
addressed by Level 4: Presentation of the current framework).
Level 3: Instructional method
The instructional method (or ‘‘mode’’) represents the specific
techniques used for learning (Gagne´ & Medsker 1996; Cook
2005). Most methods can be used no matter which simulation
modality is chosen. However, they have an important impact
on the educational experience, its perception and its effectiveness. Two instructional methods can be used with simulation: self-instruction and instructor-based learning.
Self-instruction (or self-directed learning) allows learners to
determine the timing and rhythm of learning. Learners can also
choose their desired learning objectives (Gagne´ & Medsker
1996). This method is well adapted to procedural and
computer-based simulations.
Instructor-based learning is the usual method for learning
with healthcare simulation. It requires instructor supervision
and includes varying degrees of instructor involvement, from
debriefing sessions to direct participation by the instructor in
the training session.
LeFlore and Anderson (2009) have recently studied the
effectiveness
of
teaching
sessions
in
which
instructors
modeled appropriate behaviors for the learners. They concluded that instructor-modeled learning and instructor-based
learning with debriefing were equally effective. While this
would suggest that there is a third type of instructional method,
consisting
of
observation,
this
method
is
irrelevant
to
simulation, since trainees have no hands-on experience and
do not interact with the situation. What is more, observation
relies on different learning mechanisms than simulation.
Learning theories that are appropriate for simulation but
have no bearing on learning through observation include
Kolb’s experiential learning (Kolb 1984) and deliberate practice (Ericsson 2004). In fact, from an educational standpoint,
observation constitutes a medium (level 1 in this framework)
distinct from simulation (Gagne´ & Medsker 1996).
Level 4: Presentation
Presentation includes characteristics that define exactly how
the simulation activity is shaped and designed, but that do not
constitute instructional methods per se. Although they usually
involve small differences within each instructional method,
they can have a tremendous impact on actual learning
effectiveness. Of course, the level of detail that defines the
presentation of a given simulation activity can be nearly
infinite. Yet, several characteristics are integral in the simulation experience, among which are the nature and quality of
feedback, simulation fidelity, simulator type, scenario, and
team composition (Table 2). In addition, duration of individual
training sessions and their interval certainly constitute important aspects of presentation, although they are not unique to
simulation. Finally, other factors such as the location of the
educational intervention could be important, but further
research is still required to assess their exact role.
Feedback.
Feedback is an essential characteristic of simulation training and has been identified as the single most
important presentation element for simulation, with a direct
impact on learning (Issenberg et al. 2005). It is defined as a
particular type of communication in which a sender (the
source) conveys a message to a recipient that includes
information about the recipient’s behavior (Ilgen et al. 1979).
Feedback provides the ability to reflect on the educational
experience to enhance learning (Salas et al. 2005; Fanning &
Gaba 2007). Studies of feedback in medicine, also in fields
such as psychology and management, have shown that
specific feedback has a better potential of improving performance than vague or no feedback, because it increases
knowledge about performance (Johnson et al. 1993; Hewson
& Little 1998), and, when no goals exist, allows the learner to
set specific goals for learning (Ilgen et al. 1979; Early et al.
1990). These goals in turn enhance performance by increasing
motivation, effort, and persistence, and by improving the way
strategies for success are devised (Locke et al. 1981; Early et al.
1990).
Feedback can take several forms, but its essential attributes
can be grouped into type, source, timing, the level to which
it is aimed (the individual or the group), and other attributes (including the medium through which feedback is
delivered) (Chase & Houmanfar 2009). The main characteristics will be addressed here.
The psychology and management literature usually recognizes two types of feedback: outcome feedback and process
feedback (Early et al. 1990, Johnson et al. 1993). Outcome
feedback – also called performance-oriented feedback –
G. Chiniara et al.
e1384

## Page 7

provides participants with the knowledge of their results.
While it is posited that such form of outcome would allow the
individual to improve his or her performance by altering the
strategies used to implement the task, some studies suggest
that outcome feedback is ineffective for complex, uncertain
tasks (Jacoby et al. 1984; Johnson et al. 1993).
Process feedback – also called learning-oriented, descriptive or cognitive feedback – aims at facilitating learning and has
an explanatory value (Johnson et al. 1993). It provides
descriptive information on how to perform a specific task or
on how to improve performance. Contrary to outcome
feedback, evidence suggests that process feedback improves
the strategies used to achieve an outcome, and enhances
performance, especially on complex tasks (Johnson et al.
1993). It is also likely that the effects of both types of feedback
are additive (Early et al. 1990).
In addition to its type, feedback can be classified based on
its source. There are three potential sources of feedback: the
task environment itself, the individual performing the task, and
other individuals observing the performance. The source has
an effect on the response to feedback, through the source’s
credibility and its power over sanctions and rewards imparted
to participants (Ilgen et al. 1979).
The first source of feedback is the task and its environment.
By its very nature, feedback is an inherent characteristic of
simulation, even when no external feedback is provided. The
appropriate replication of physical characteristics provides
guidance to the user, called task-inherent feedback or natural
feedback (Friedman 1995). This occurs, for example, when an
appropriate technique leads to success upon inserting a
thoracic drain, or when inappropriate management decisions
lead to worsening of the patient’s condition. Such feedback
emanating from the task environment can be enhanced by
additional mechanisms that supplement the information provided through natural feedback. This type of feedback, called
augmented feedback (Ilgen et al. 1979) is often embedded into
virtual reality simulators.
Another source of feedback is the individual performing
the task, who can provide his or her own feedback on the
processes involved in the performance or its outcomes.
However, studies do not generally support the ability of
individuals to self-assess (Davis et al. 2006), although selfTable 2.
Important elements of presentation for simulation experiences, along with their major characteristics and descriptors.
Presentation element
Characteristic
Descriptors
Feedback
Type
Outcome feedback: Knowledge of the results or outcomes of the individuals’ actions.
Process feedback: Feedback providing descriptive information of the processes performed
and required, aimed at facilitating learning.
Source
The task itself: Feedback provided by the simulator responding realistically to trainees’
actions (natural feedback) or expanded to convey additional information (augmented
feedback).
An observer (expert-directed feedback): Feedback provided by an external subject-matter
expert having observed the performance. This often takes place during debriefing
sessions.
Other learners (peer feedback): Feedback provided by other learners involved in the
educational experience.
The learner (self-feedback): Self-assessment of one’s performance
Timing
Synchronous: Feedback provided during simulation
Immediate: Feedback provided immediately after simulation
Delayed: Feedback provided after a delay
Fidelity
Patient (physical) fidelity
Describes how closely the patient feature reproduced in the simulation matches the ‘‘real
world’’ feature.
Two metrics can be used to measure and report patient fidelity.
Resolution: Whether a required feature from the real world is reproduced in simulation.
Accuracy: The degree to which a feature of real world is adequately reproduced in
simulation.
Environment fidelity
Describes how closely the environment feature reproduced in the simulation matches the
‘‘real world’’ feature.
Two metrics can be used: resolution and accuracy.
Temporal fidelity
Describes how closely time flows in the simulation compared to reality.
Two metrics can be used: resolution and accuracy.
Simulator type
See Table 3
Scenario
Design
Team
Composition
Actors
Single discipline
Interdisciplinary (interprofessional)
Work unit
Other
Duration of training sessions
Intervals of training sessions
Location
Dedicated setting
In situ
A framework for simulation in healthcare
e1385

## Page 8

feedback could have value as a motivational or development
tool (Campbell & Lee 1988).
External subject-matter expert observers can provide feedback called expert-directed feedback. In healthcare simulation,
this often takes the form of after-the-fact debriefing sessions
(Dreifuerst 2009). Such sessions can be conducted using
several styles or models of feedback, such as non-judgmental
debriefing (Rudolph et al. 2006), plus-delta (Fanning & Gaba
2007)
or
target-focused
feedback
(Wallin
et
al.
2007).
Unfortunately, few published studies have described the
effectiveness of various styles of debriefing (Fanning & Gaba
2007). It is beyond the scope of this article to describe those
styles, and readers are referred to the appropriate references.
Finally, feedback can be provided by the other learners
involved in the training experience. Such peer feedback may
be integrated in the debriefing session. When used as a
learning method (rather than an assessment) it can be
particularly useful for providing insight into one’s performance
(Falchikov 1995), and has been successfully used in areas of
healthcare such as nursing, especially in pregraduate training
(Morris 2001).
Timing is also an important element of feedback. When
feedback is provided during simulation, it is described as
synchronous. Feedback can also be immediate, when it is
given after each simulation session (e.g., debriefing after each
scenario during simulated clinical immersion), or delayed
when there is a time gap between the end of the simulation
session and feedback. There is some evidence that, while
synchronous feedback is appropriate for procedural simulation, immediate feedback is more appropriate when the aim of
simulation is to integrate a particular concept or model (Scha¨r
et al. 2000; Williams 2003). Furthermore, synchronous feedback, when it is provided by an external source, could intrude
on the learning experience (Williams 2003). Finally, delayed
feedback, although often more feasible, may not be as
effective as synchronous or immediate feedback (Salas et al.
2008).
Other elements that might determine the effectiveness of
feedback include duration of the feedback session relative to
the simulation session, and the provision of individual and
team-oriented feedback when appropriate (Salas et al. 2008).
Fidelity.
Fidelity, defined as the realism of the experience, is
an intrinsic characteristic of simulation and an element of
presentation that can affect learning (Issenberg et al. 2005;
Gaba 2007). As such, it is important to be able to define and
measure it. In 1999, the Fidelity Implementation Study Group
formed
by
the
Simulation
Interoperability
Standards
Organization (SISO) presented a first report describing some
of the major conceptual frameworks for fidelity and sketching
a fidelity taxonomy (Gross 1999). The report highlighted the
difficulties in establishing standards for fidelity in simulation.
Still, it argued, ‘‘if we are to make fidelity a useful concept, then
we must make it measurable.’’ Simply using broad descriptors
such as ‘‘high fidelity’’ or ‘‘low fidelity’’ is not enough and is
misleading since fidelity is a multidimensional construct
(Rehmann et al. 1995; Maran & Glavin 2003; Beaubien &
Baker 2004). In order to make fidelity measurable, four steps
are required: (1) articulation of the standard, (2) identification
of the fidelity taxonomy, (3) measurement of fidelity features,
and (4) determination of the required level of fidelity.
The standard against which fidelity should be measured
(the ‘‘fidelity referent’’) is not the real world. The latter is too
large and impossible to adequately describe. More importantly, most of the real world would be irrelevant to a
particular learning outcome. Instead, the fidelity referent
should include the minimal characteristics of real world
features that are needed for a given educational experience
(Gross 1999). For example, an arm used for the insertion of a
peripheral venous catheter might have adequate fidelity if the
position and aspects of the veins as well as the feel of the
techniques performed are realistic, independently of other
features such as color of the skin or the ability of the simulator
to react to pain. Of course, if the objectives of a training session
also include communication skills, then some aspects of
fidelity would be lacking. Hence, the fidelity referent should
be established by closely analyzing the desired learning
outcomes and matching them to the real world features. This
would usually entail a thorough (and, ideally, standardized)
description of the target domain (Gross 1999).
The second step involves determining the appropriate
dimensions and features of fidelity to analyze. In the field of
aviation, important dimensions include physical fidelity, visual
fidelity, audio, motion, environment, temporal fidelity, behavior and aggregation (Gross 1999). Two of these, physical and
environment fidelity, have been used in healthcare simulation
(Maran & Glavin 2003; Beaubien & Baker 2004). Behavior
fidelity (the way features of the simulation react), while
important, can be subsumed under these two domains.
Physical fidelity in healthcare simulation refers to the realism
of the patient or of the component that is simulated. As such, it
could also be called patient fidelity. Environment fidelity refers
to the realism of all elements not directly connected to the
patient, including the setting and the personnel. To these two
domains, we should add temporal fidelity, which refers to the
way time flows during the simulation session. At the high end,
time flows unimpeded. At the low end, temporal contractions
or pauses take place.
The third step is to measure the agreement between
features of the simulation and the fidelity referent. One fidelity
framework describes two metrics: resolution and accuracy
(Gross 1999). Resolution, a dichotomous value, refers to
whether the feature of the referent is reproduced in the
simulation. Accuracy refers to the degree to which the feature
of the referent is closely reproduced in the simulation. It would
be expressed through a numeric index, but the actual
measures used to generate it vary. Several techniques should
be used depending on the specific feature, including error
estimation techniques. Of course, the complexity increases
tremendously when a human element is added to the
simulation, which may require resorting to an analysis by
subject matter experts (Gross 1999).
The final step for analyzing fidelity is to determine the
appropriate and required level of both fidelity metrics. This
determination will depend on the targeted learning outcomes
(Salas et al. 1998) and on the participants’ level of expertise
(Maran & Glavin 2003). However, studies using a strict
definition of fidelity are lacking (Cook et al. 2011), and the
G. Chiniara et al.
e1386

## Page 9

issue of the extent of fidelity required for a given educational
experience is still unresolved.
Fidelity is not inherent to the specific simulation setting
used, but varies from one ‘‘case’’ to another. Moreover, fidelity
metrics do not apply broadly to a fidelity category or domain
(such as the environment), but to individual elements from
each domain that are essential to the specific educational
experience. Although the process of determining fidelity is
expensive and time-consuming, it is necessary both to
ascertain the quality of the simulation activity and to standardize research. Terms such as ‘‘high fidelity mannequin’’ or ‘‘high
fidelity
simulation’’
are
overreaching
and
misleading.
Unfortunately, even in a field with extensive experience in
simulation, such as aviation, fidelity is still lacking precise
metrics (Rehmann et al. 1995).
Simulator type.
The type of simulator can alter the effectiveness of the learning experience, partly through its effects
on the available instructional methods and on other aspects
of presentation such as fidelity and feedback. Available
simulators depend mostly on the chosen simulation modality,
and
several
simulators
can
be
chosen
for
more
than
one
modality.
Table
3
provides
a
definition
for
each
simulator type.
Organic simulators (animals, tissues or cadavers) and
synthetic simulators (including the so-called ‘‘part-task trainers’’ as well as patient simulators when used for this purpose)
are mostly used for procedural simulation. Virtual reality
simulators are types of synthetic simulators where interfacing
with the simulator is done through highly realistic (natural)
means, and a computer controls nearly all the outputs of
simulation events (Gorman et al. 1999; Kaufmann & Liu 2001).
Computer-based simulation, as a modality, can make use
of a more limited set of simulator types. These include virtual
patients, virtual worlds, and computer (or web) applications.
Virtual patients are becoming increasingly important in
healthcare simulation (Cook & Triola 2009; Huwendiek
et al. 2009). They enable health learners to practice a large
number of medical skills, including communication and
history taking (Bernard et al. 2006). Virtual worlds that
replicate complete environments and diverse events on a
computer-screen have also seen a surge in popularity. They
use platforms dedicated to education in healthcare such as
Virtual ED, a virtual emergency department (Youngblood
et al. 2008), gaming platforms including environments dedicated to ‘‘massively multiplayer online games’’ or MMOG
(Youngblood et al. 2007), or virtual communities such as
Second Life (Linden Lab, San Francisco) (Phillips & Berge
2009). Computer-based simulations can also involve computer applications that mimic the functions of real-world
systems, such as web-based anesthesia machine simulators
(Lampotang 2003).
Simulated clinical immersion and SPs can be designed
using multiple simulator types, including actors, patients,
patient simulators, and any of the simulators usually used for
technical skills training. This is an important point when
discussing simulator types. Most of the terminologies used
today in the literature, such as ‘‘mannequin-based simulation’’
(Tsai et al. 2006) or ‘‘high-technology patient simulation’’
(Lareau et al. 2010), emphasize the patient simulator (the
‘‘mannequin’’) to the detriment of other aspects of the
experience. Not only does this put the emphasis on the
technology rather than the educational experience, it also
suggests a specific role for the patient simulator in learning.
Yet, the learner’s actions that are part of the educational
experience are similar, whether the patient is reproduced by
an actor, an actual patient, another learner, or a patient
simulator (Collins & Harden 1998).
Table 3.
Descriptions and examples of available simulator types.
Type of simulator
Description
Example
Actor
An individual who takes on a specific role during a simulation session.
An actor can be a paid individual, a confederate, or another learner
(role-playing).
A paid individual that plays the role of a family
member during a simulation session on communication skills.
Computer (or web)
application
A computer-based program, delivered either locally or through the
Internet, that reproduces, in whole or in part, actual systems or
equipment.
Virtual Anesthesia Machine (University of Florida)
Organic
A procedural simulator that uses organic material for skills training.
Animals
Human cadavers
Organic tissue
Part-task trainer
A synthetic simulator that replicates specific components of a patient or
a system, for skills training.
TraumaMan System (Simulab Corporation)
Advanced Catheterization Trainer (Limbs & Things)
Patient
An actual patient who takes on his or her own role during a simulation
session.
Patient simulator
A life-size mannequin representing a patient, which can simulate several
behaviors and characteristics of an actual patient.
iStan (CAE Healthcare)
SimMan 3G (Laerdal Medical)
Noelle (Gaumard Scientific)
Virtual patient
A computer-based program that allows the learner to interact with
a pre-programmed patient through a screen-based interface
(i.e., computer screen)
Virtual reality
A synthetic simulator used for skills training, which provides the learner
with a realistic interface and outputs the results through a computer.
Virtual I.V.TM (Laerdal Medical)
LAP MentorTM (Symbionix)
Virtual world
A computer-based program that allows the learner to be immersed,
through a screen-based interface (i.e., a computer screen), in the
digital recreation of an environment or setting. The learner often
interacts with the simulation through a digital persona, or ‘‘avatar’’.
Second Life (Linden Lab)
Massively Multiplayer Online Games (MMOG)
Virtual ED (Stanford University)
A framework for simulation in healthcare
e1387

## Page 10

Scenario.
The scenario used for simulation, which describes
the patient case, is an important aspect for enhancing learning.
Salas and collaborators have suggested that the ‘‘scenario is the
curriculum’’ (Salas et al. 2005, p. 364). It is a crucial
presentation element since it serves as the foundation upon
which learning will take place (Salas et al. 1998; Salas & Burke
2002). It must be based on the intended learning outcomes. It
must have different levels of difficulty and allow the learners a
certain degree of control over the events.
Team composition.
The last presentation of interest is team
composition. It has a direct impact on learning since simulation
is often a shared, social experience, contrary to other
instructional media (Dieckmann et al. 2007). Aside from
actors and confederates, teams can be composed of members
of a single discipline. They can alternatively involve interdisciplinary (or interprofessional) units, and work units. In single
discipline teams, each team member plays a role required by
the actual scenario, whether the role is within or outside his or
her professional domain. In interdisciplinary units, each team
member plays a role consistent with his or her professional
domain. As for work units, they consist of actual clinical teams.
Interdisciplinary teams are often favored for learning attitudes
and beliefs related to patient safety and teamwork (BuljacSamardzic et al. 2010). Yet single discipline teams could
provide their members with unique insights into other
professionals’ roles, and be a major impetus for changes in
culture and behavior within multidisciplinary clinical teams.
Work units may increase transfer of knowledge to the actual
clinical setting, although this assertion needs to be supported
by future research.
ID in simulation and the outcomes
of learning
Instructional media and simulation modalities are each best
adapted for specific uses. Indeed, as Gagne´ and his collaborators asserted: ‘‘Technology is not an end in itself; any
successful use of training technology must begin with clearly
defined educational objectives’’ (Gagne´ et al. 2005, p. 231).
The intended knowledge, skills, and attitudes (and often
learners’ expertise level) dictate the modality best adapted for
each learning curriculum and training session (Table 4).
In recent years, healthcare education has moved away from
objective-centered
approaches,
to
more
outcome-based
models for education (Harden 2007). Outcome-based education (OBE) highlights the importance of learning outcomes and
competencies in designing curricula, particularly for choosing
appropriate
instructional
methods
(Harden
2007).
While
learning outcomes and competencies can be used interchangeably, the latter typically refers to the end-points of learning
expected from the learners, whereas the former also relates to
programs (Ellaway et al. 2007).
Several learning outcomes frameworks have been developed for local, national, or international use. Some of the best
known systems in medicine are the CanMEDS competency
framework developed by the Royal College of Physicians and
Surgeons of Canada (Frank 2005), the ACGME outcome
project developed by the Accreditation Council for Graduate
Medical Education in the US (Swing 2007), and Tomorrow’s
Doctors developed by the General Medical Council in the UK
(General Medical Council 2009). Other healthcare professions
have also developed similar systems, and interprofessional
competency
frameworks
have
been
published
(CIHC
Competencies Working Group 2010). Finally, some frameworks have been developed to highlight the importance of
training in a subset of competencies that are often neglected or
ignored. Thus, in Canada, a framework complementary to the
CanMEDS model has been published by a licensing body and
by a patient-advocate organization, which specifically targets
learning outcomes aimed at improving patient safety (Frank &
Brien 2008).
In discussing learning outcomes for simulation-oriented
training, these published frameworks are too broad (or, in
some cases, too specific) to be of practical use. Moreover, most
of them are aimed at specific healthcare professions, such as
medicine, while the intent of the model described here is
interprofessional. Hence, we have opted to discuss specifically
the following interprofessional competency domains: (1) rote
knowledge; (2) techniques and procedures; (3) history,
physical exam and patient counseling; (4) clinical reasoning
and patient management; (5) teamwork and crisis management; (6) ethics and beliefs. These domains by no means cover
the scope of the healthcare professions, but highlight some
of the most salient knowledge, skills, and attitudes that
constitute the health learning continuum, are useful for ID,
and are consistent with the way others have approached
Table 4.
Adaptation of simulation modalities to specific learning outcomes domains.
Simulated
clinical
immersion
Procedural
simulation
Computer-based
simulation
SP
Rote knowledge
–
þ
þþþ
þ
Techniques and procedures
þ
þþþ
þþ
þ
History, physical exam and patient counseling
þ
–
þ
þþþ
Clinical reasoning and patient management
þþþ
–
þþ
þþþ
Teamwork and crisis management
(patient safety competencies)
þþþ
–
þþ
þ
Ethics and beliefs
þþ
þ
þ
þþ
Note: – not adapted; þ weakly adapted; þþ moderately adapted; þþþ strongly adapted.
G. Chiniara et al.
e1388

## Page 11

simulation-based learning (Cook & Triola 2009). It is of course
possible to map those domains to existing frameworks, and
local educators and program developers who wish to implement simulation-based training should cross-reference those
domains to the learning outcomes frameworks that are
appropriate to their programs. Examples and discussions of
methodologies for cross-referencing different learning outcomes frameworks can be found in the appropriate literature
(Ellaway et al. 2007).
Although rote knowledge is best achieved through other
media, SPs could be used to that end, especially where clinical
knowledge is concerned. SPs, however, are particularly
adapted for a wide array of competencies that require direct
contact with a patient, including history taking, physical
examination, and patient counseling (Cleland et al. 2009).
Along with clinical reasoning, situation awareness – the
ability to ‘‘read’’ the ongoing situation and predict its evolution
(Endsley 1995) – is an important aspect of clinical management. While controversy exists regarding their nature as actual
skills (Patrick & James 2004; Eva 2005; Norman 2005), there is
still widespread agreement that they must be developed by
learners of health sciences (Schuwirth 2002; Patrick & James
2004). SPs and simulated clinical immersion are both appropriate modalities for developing competencies such as clinical
reasoning and patient management. However, the high costs
associated with simulated clinical immersion and the increased
cognitive burden that interacting with the environment adds
on the learner suggest that it should probably be reserved for
situations where the environment is an important contributing
factor in training the intended competencies, such as training
in situation awareness, training in patient management in a
highly dynamic situation that imposes time constraints and
increases the stakes of both diagnosis and treatment, or
training in error management (Gaba et al. 2001; Hassan &
Sloan 2006). Concurrently, simulated clinical immersion has
been shown to be effective for training in safety competencies,
especially in dynamic or crisis situations (Wallin et al. 2007;
Buljac-Samardzic et al. 2010), and is often used for that
purpose (Raemer 2004; Wayne et al. 2008; Smith & Cole 2009).
This domain includes several competencies related to ‘crisis
management’ or ‘non-technical skills’, such as team management skills (Fletcher et al. 2004; Reader et al. 2006), which are
seldom described in the medical education literature. These
competencies can be subsumed under the heading patient
safety competencies (Frank & Brien 2008). They include skills
such as role clarity, communication and resource utilization,
which are usually mobilized within entire teams during crisis
situations (Raemer 2004).
The acquisition and development of specific perceptual
and motor skills, the proper performance of techniques, and
the application of their underlying procedures are important
skills in healthcare, and are usually targeted extensively
through procedural simulation. Self-instruction is possible,
especially
through
the
use
of
virtual
reality
simulators
(Moorthy et al. 2003) or, when motor practice is not necessary,
through computer-based simulation. It is necessary, however,
to realize that beliefs and attitudes underlying the specific
procedures are paramount. For example, dedication to the
adherence of proper protocol, especially in the face of time
constraints and pressures that are often an integral part of
health professions, is often as important as knowing the
appropriate steps in a procedure. As such, simulated clinical
immersion and SPs can be a useful adjunct to procedural
simulation, since they allow the replication of the actual
clinical conditions that prevail in the learners’ workplace.
Appropriate beliefs and attitudes, including ethical conduct,
are not unique to technical procedures but permeate the
continuum of healthcare and are recognized by all major
learning outcomes frameworks. Given their close resemblance
to actual patient care, SPs and, especially, simulated clinical
immersion are valuable learning modalities for developing and
maintaining these attitudes and beliefs. SPs have indeed been
used for teaching clinical ethics (Edinger et al. 1999), but there
still is a paucity of data, especially related to the use of
simulated clinical immersion in affecting beliefs and attitudes.
One study has shown clinical immersion to be ineffective in
changing beliefs toward patient safety (Wallin et al. 2007).
This, however, could reflect the inherent difficulty in changing
beliefs and attitudes, or it could be an artifact of the assessment
method used.
Depending on the type of simulator used, computer-based
simulation can be adapted to a wide set of learning outcomes.
Computer or web applications are a great asset for learning
basic knowledge (Chumley-Jones et al. 2002). Virtual patients
are effective for teaching clinical reasoning and patient
management in non-dynamic situations (Triola et al. 2006),
and may be the modality of choice for these domains (Cook &
Triola 2009), although a recent meta-analysis showed no
difference with other, non-computer instruction (Cook et al.
2010). Virtual worlds have shown promise in developing
teamwork and crisis management skills, and have proven, in
one pilot study, equally as effective as simulated clinical
immersion (Youngblood et al. 2008). Computer-based simulators are also effective for learning or developing skills and
procedures, especially in novice learners, before hands-on
experience is introduced (Vukanovic-Criley et al. 2008).
Choosing the appropriate media
and simulation modalities
Media selection charts are useful adjuncts to learning theories
for ID (Reiser & Gagne´ 1983; Gagne´ & Medsker 1996).
Previously published charts were dependent on learner
characteristics, objectives, and prior decisions made about
the instructional methods. We present here a chart inspired by
Robert Gagne´’s seminal work (Reiser & Gagne´ 1983), and
adapted to simulation (Figures 4 and 5). It is based on learning
outcomes as a first level, rather than a priori decisions about
the instructional mode, as in Gagne´’s chart.
Determining desired learning outcomes for a given learning
activity, based on the learners’ profession, on local practices,
and on published competency frameworks is a fundamental
step in ID. Then, through a series of questions provided in the
charts (and by referencing Table 4 if needed), the educator can
choose the suitable medium and simulation modalities (when
appropriate) in order to foster better learning and to achieve
the outcomes set forth in the curriculum.
A framework for simulation in healthcare
e1389

## Page 12

Figure 4.
Media and simulation modalities selection chart A (continued in Figure 5). Diamond shapes correspond to key
decision points (questions). Rectangles with dark borders correspond to appropriate media and simulation modalities. Text in red
and bold corresponds to preferred modalities. AND/OR decision points suggest that several competency domains can be
considered at the same time (potentially leading to the use of hybrid modalities). CBS: Computer-Based Simulation; pt: Patient;
SCI: Simulated Clinical Immersion; SP: Simulated Patient; VR: Virtual Reality.
G. Chiniara et al.
e1390

## Page 13

Figure 5.
Media and simulation modalities selection chart B (continued from chart A in Figure 4). Diamond shapes correspond
to key decision points (questions). Rectangles with dark borders correspond to appropriate media and simulation modalities. Text
in red and bold corresponds to preferred modalities. AND/OR decision points suggest that several competency domains can be
considered at the same time (potentially leading to the use of hybrid modalities). CBS: Computer-Based Simulation; PE: Physical
exam; Hx: History; pt: Patient; SCI: Simulated Clinical Immersion; SP: Simulated Patient.
A framework for simulation in healthcare
e1391

## Page 14

We acknowledge that this chart is a work in progress
and will need to be expanded by future research and
publications. This could include refining the level of detail
by adding aspects of presentation, informed by the published
research.
Discussion
There is a great need, in the nascent field of healthcare
simulation, for an ID framework. A framework is necessary to
structure future research, and provide direction to educators
and simulation proponents in the design and creation of
effective and innovative curricula that optimize what simulation has to offer. Such an instructional framework has to be
flexible enough to ensure it can meet the wide variety of
training demands it must support, which include technical
skills development (airway management, lumbar puncture,
etc.), non-technical skills (teamwork, communications, leadership), train-the-trainer courses (debriefing, creation of learning environments), or specialized skills (hospital evacuation,
mass casualty management). We believe the framework
presented here achieves these goals and further, it can be
easily implemented in both ID and research.
Simulation, by itself, is not a guarantee that adequate
learning will occur. Educational sessions using simulation must
rely on appropriate ID based on learning theories (Salas et al.
2005). The framework presented here can help in designing
such effective simulations.
The framework classifies characteristics of the educational
experience into four levels: medium, simulation modality,
educational method, and presentation. It offers guidance to
adapt each level to the appropriate educational goals and
objectives, and provides grounds for evidence-based practices
in ID. The designer of a training course can determine whether
simulation constitutes an appropriate medium for delivery of
instruction by mapping the severity and likelihood of exposure
of the desired events to the zone of simulation matrix. The
exact simulation modality can in turn be determined by the
desired learning outcomes and through the use of the media
and modality selection charts. After determining the required
instructional methods, the program designer would then give
specific attention to the items of presentation which include
type of simulator, fidelity, feedback, scenario, and so on. This
tool is thus useful for the simulation educator that aims to
develop effective training sessions.
The framework can further be used to direct research
design. In his paper describing the framework for e-learning,
David Cook makes a persuasive argument that mediacomparative research – that is, research comparing the
learning effectiveness of two different media – is logically
impossible given that there are no valid comparison groups
(Cook 2005). Other authors had previously made such claims
(Keane et al. 1991; Clark 1992; Friedman 1994). These authors
argue that the diverse elements involved in each instructional
medium introduce confounding variables when comparing
two groups in which different media are used. Instead, they
argue that it is best to compare within a single medium the
features that promote optimal learning, in order to inform the
field of education. Cook extended this claim by arguing that
research for e-learning is best done within each level of his
four-level framework rather than between levels. While a full
discussion of this issue is beyond the scope of this article, we
would argue that the same holds true for simulation. By
providing a stepwise framework of simulation features, we
hope that future research can be tailored to determine the
specific features of simulation that are best conducive to
learning by designing studies that compare elements within
each level of the framework rather than across it.
In developing this model, it was inevitable that a taxonomy
be adapted to its use. We believe this taxonomy will be useful
for the educators and users of simulation alike, and it is our
hope that the taxonomy be broadly adopted. Other taxonomies have been developed elsewhere. One such effort was
conducted by Alinier (2007) who described a fairly extensive
and valuable typology of simulation tools. In contrast to this
taxonomy, however, our model does not focus on technology,
but on the educational experience that is the end-product of
the ID process. As was discussed earlier, there is no inherent
characteristic in a patient mannequin that would uniquely
affect the final learning outcomes compared, for example, to
an actual patient, provided that the tool used (simulator or
patient) allows adequate reproduction and training of the
targeted competencies. What is more, a drawback of a
technology-focused taxonomy is that it often assigns important
presentation attributes, such as fidelity, to the tool itself (the
simulator) rather than to the pedagogical end-point, the
individual educational experience.
This framework has several limitations. While it rests on
solid theoretical grounds and relies on published studies, there
is still a paucity of available literature specifically aimed at best
practices for ID in healthcare simulation. Many of the concepts
presented here are grounded in experience and still need to be
formally studied and supported. As such, several other
presentation elements (such as group size), which probably
have an effect on the educational experience, still need to be
researched and analyzed. The media selection charts could be
expanded in the future (e.g., by adding presentation aspects
such as fidelity) in order to improve the model. Given the
available literature, it is impossible for such a framework to be
entirely prescriptive, although future studies may refine it and
move it closer to a fully-prescriptive model.
What is more, the terminology used is bound to be
somewhat idiosyncratic. While the authors are educators and
simulation experts from across Canada, we acknowledge that
local practices may affect the way some definitions and terms
are used, especially when the literature is not conclusive on
their use.
Finally, the conceptual framework presented here is seen
through the lens of education. It is explicitly presented as an
ID framework with clearly defined parameters (simulation
centered on a patient). As such, it does not address some
aspects of simulation that are indubitably very important
endeavors, such as epistemology of human factors and
systems dynamics. Such efforts should be grounded in other
frameworks and approached differently.
G. Chiniara et al.
e1392

## Page 15

Conclusion
We have presented a framework for ID in healthcare simulation that provides the simulation educator with tools to
appropriately design training sessions based on learning
outcomes and instructional intent. The framework is grounded
in previously published studies, although many more studies
are needed to refine it.
Despite some limitations, this framework fills a void in the
area of ID and research for healthcare simulation. The CNSH
hopes that the model provided in this article will be adopted
by simulation proponents in Canada and elsewhere, in order
to design effective curricula and standardize research. It if
further our hope that the framework can serve as a catalyst for
the simulation community – which includes clinicians, educators, and experts in other fields – to engage in a discussion
about the educational characteristics of simulation and to
encourage future research in this field.
Acknowledgments
The authors acknowledge the invaluable contribution of Mr
Murray Doggett, both for providing clerical support and for
offering important feedback. The authors also recognize the
insightful input of Ms Jacqueline Lyndon. Finally, the authors
thank two anonymous reviewers whose insights were very
helpful in improving this article.
Declaration of interest: The authors report no conflicts of
interest. The authors alone are responsible for the content and
writing of this article.
Notes on Contributors
THE CNSH GUIDELINES WORKING GROUP: The Guidelines Working
Group was created by the Canadian Network for Simulation in Healthcare
for the purpose of developing and promoting standards and guidelines for
best practices in simulation-based learning. It is composed of clinicians,
educators, simulation experts and managers from across Canada, from a
wide diversity of healthcare professions.
GILLES CHINIARA, M.D., FRCPC, MHPE, is an Anesthesiologist and an
Assistant Professor at Universite´ Laval. He is currently the Director of Centre
Apprentiss, the first simulation center in the province of Quebec, which he
helped design. He has received many local and national awards for his
involvement in education and simulation.
GARY COLE, PhD, is presently Senior Researcher/Developer at the Royal
College of Physicians and Surgeons of Canada and adjunct professor at the
University of Calgary. His primary interest is Evaluation. He has used the
Harvey simulator for evaluation in high stakes situations and in comparison
to the evaluation of real patients.
KEN BRISBIN, EMT, CD, is an Emergency Medical Technician, former
Canadian Forces Tank Commander and Firefighter, with a background in
Adult Education. He currently works as a Simulation Consultant for Alberta
Health Services, where he assists and advises all health professions within
AHS in utilizing simulation as part of their continuing education.
DAN HUFFMAN, BN, MSA, MMM, CD, has over 30 years experience in the
military and civilian health industry. Dan’s experience includes leadership
roles in Disaster Response Planning, Flight Safety, Aeromedical Evacuation,
Telehealth and Simulation. Dan is currently the Director of the Alberta
Health Services Simulation program that supports training of over 90,000
staff and physicians.
BETTY CRAGG, RN, EdD, Betty Cragg, EdD RN, is a Professor Emeritus and
former Director of the School of Nursing, University of Ottawa. She
continues to do research on simulation and interprofessional education.
MIKE LAMACCHIA, EMT-P, CMTE, works for the Shock Trauma Air Rescue
Society (STARS) based out of Calgary, Alberta, Canada, and is the Vice
President of Operations-Alberta and Chief Clinical Officer. Primary
responsibilities include leadership, direction and oversight of all Albertabased operations, including development and monitoring of STARS clinical
standards.
DIANNE NORMAN, RN, BScN, MEd, is the Clinical Outreach Specialist
at McMaster Children’s Hospital where she provides leadership to the
hospital-based simulation program.
References
Alinier G. 2007. A typology of educationally focused medical simulation
tools. Med Teach 29:e243–e250.
Appelman R. 2004. Designing experiential modes: A key focus for
immersive learning environments. TechTrends 49:64–74.
Barrows HS. 1993. An overview of the uses of standardized patients for
teaching and evaluating clinical skills. Acad Med 68:443–451.
Beaubien JM, Baker DP. 2004. The use of simulation for training teamwork
skills in health care: How low can you go? Qual Saf Health Care
13(Suppl 1):i51–i56.
Bernard T, Stevens A, Wagner P, Bernard N, Oxendine C, Johnsen K,
Dickerson R, Raji A, Lok B, Duerson M, et al. 2006. A multi-institutional
pilot study to evaluate the use of virtual patients to teach health
professions students history-taking and communication skills. Simul
Healthc 1:92.
Bokken L, Rethans JJ, Scherpbier AJ, van der VlC. 2008. Strengths and
weaknesses of simulated and real patients in the teaching of skills to
medical students: A review. Simul Healthc 3:161–169.
Buljac-Samardzic M, Dekker-van Doorn CM, van Wijngaarden JD, van Wijk
KP. 2010. Interventions to improve team effectiveness: A systematic
review. Health Policy 94:183–195.
Campbell DJ, Lee C. 1988. Self-appraisal in performance evaluation:
Development versus evaluation. Acad Manage Rev 13:302–314.
Chase J, Houmanfar R. 2009. The differential effects of elaborate feedback
and basic feedback on student performance in a modified, personalized
system of instruction course. J Behav Educ 18:245–265.
Chumley-Jones HS, Dobbie A, Alford CL. 2002. Web-based learning: Sound
educational method or hype? A review of the evaluation literature. Acad
Med 77:S86–S93.
CIHC Competencies Working Group. 2010. A national interprofessional
competency
framework.
Vancouver,
Canada:
Canadian
Interprofessional Health Collaborative.
Clark RE. 1992. Dangers in the evaluation of instructional media. Acad Med
67:819–820.
Cleland JA, Abe K, Rethans JJ. 2009. The use of simulated patients in
medical education: AMEE Guide No 42. Med Teach 31:477–486.
Collins JP, Harden RM. 1998. AMEE Medical Education Guide No. 13: Real
patients, simulated patients and simulators in clinical examinations.
Med Teach 20:508–521.
Cook DA. 2005. The research we still are not doing: An agenda for the
study of computer-based learning. Acad Med 80:541–548.
Cook DA, Erwin PJ, Triola MM. 2010. Computerized virtual patients in
health professions education: A systematic review and meta-analysis.
Acad Med 85:1589–1602.
Cook DA, Hatala R, Brydges R, Zendejas B, Szostek JH, Wang AT, Erwin PJ,
Hamstra SJ. 2011. Technology-enhanced simulation for health professions education: A systematic review and meta-analysis. JAMA 306:
978–988.
Cook DA, Triola MM. 2009. Virtual patients: A critical literature review and
proposed next steps. Med Educ 43:303–311.
Davis DA, Mazmanian PE, Fordis M, Van Harrison R, Thorpe KE, Perrier L.
2006. Accuracy of physician self-assessment compared with observed
measures of competence: A systematic review. JAMA 296:1094–1102.
Dieckmann P, Gaba D, Rall M. 2007. Deepening the theoretical foundations
of patient simulation as social practice. Simul Healthc 2:183–193.
Dreifuerst KT. 2009. The essentials of debriefing in simulation learning: A
concept analysis. Nurs Educ Perspect 30:109–114.
A framework for simulation in healthcare
e1393

## Page 16

Early PC, Northcraft GB, Lee C, Lituchy T. 1990. Impact of process and
outcome feedback on the relation of goal setting to task performance.
Acad Manage J 33:87–105.
Edinger W, Robertson J, Skeel J, Schoonmaker J. 1999. Using standardized
patients to teach clinical ethics. Med Educ Online 4:1–5.
Ellaway R, Evans P, McKillop J, Cameron H, Morrison J, McKenzie H,
Mires G, Pippard M, Simpson J, Cumming A, et al. 2007. Crossreferencing the Scottish Doctor and Tomorrow’s Doctors learning
outcome frameworks. Med Teach 29:630–635.
Endsley MR. 1995. Toward a theory of situation awareness in dynamic
systems. Hum Factors 37:32–64.
Ericsson KA. 2004. Deliberate practice and the acquisition and maintenance
of expert performance in medicine and related domains. Acad Med
79:S70–S81.
Eva KW. 2005. What every teacher needs to know about clinical reasoning.
Med Educ 39:98–106.
Falchikov N. 1995. Peer feedback marking: Developing peer assessment.
Innov Educ Train Int 32:175–187.
Fanning RM, Gaba DM. 2007. The role of debriefing in simulation-based
learning. Simul Healthc 2:115–125.
Fletcher G, Flin R, McGeorge P, Glavin R, Maran N, Patey R. 2004. Rating
non-technical skills: Developing a behavioural marker system for use in
anaesthesia. Cogn Tech Work 6:165–171.
Frank JR, ed. 2005. The CanMEDS 2005 Physician Competency Framework.
Better standards. Better physicians. Better care. Ottawa, Canada: The
Royal College of Physicians and Surgeons of Canada.
Frank JR, Brien S. 2008. The Safety Competencies Steering Committee. The
safety competencies: Enhancing patient safety across the health
professions. Ottawa, Canada: Canadian Patient Safety Institute.
Friedman CP. 1994. The research we should be doing. Acad Med 69:
455–457.
Friedman CP. 1995. Anatomy of the clinical simulation. Acad Med
70:205–209.
Gaba DM. 2007. The future vision of simulation in healthcare. Simul
Healthc 2:126–135.
Gaba DM, Howard SK, Fish KJ, Smith BE, Sowb YA. 2001. Simulation-based
training in anesthesia crisis resource management (ACRM): A decase of
experience. Simul Gaming 32:175–193.
Gagne´ RM, Medsker KL. 1996. The conditions of learning: Training
applications. Belmont, CA: Wadsworth.
Gagne´
RM, Wager WW, Golas KC, Keller JM. 2005. Principles of
instructional design. 5th ed. Belmont, CA: Wadsworth.
General Medical Council. 2009. Tomorrow’s doctors: Outcomes and
standards for undergraduate medical education. London: GMC.
Gorman PJ, Meier AH, Krummel TM. 1999. Simulation and virtual reality in
surgical education: Real or unreal? Arch Surg 134:1203–1208.
Gross DC. 1999. Report from the Fidelity Implementation Study Group.
Orlando, FL: Simulation Interoperability Standards Organization,.
Harden RM. 2007. Outcome-based education – The ostrich, the peacock
and the beaver. Med Teach 29:666–671.
Hassan
Z-U,
Sloan
P.
2006.
Using
a
mannequin-based
simulator
for anesthesia resident training in cardiac anesthesia. Simul Healthc
1:44–48.
Hewson MG, Little ML. 1998. Giving feedback in medical education:
Verification of recommended techniques. J Gen Intern Med 13:111–116.
Huwendiek S, De leng BA, Zary N, Fischer MR, Ruiz JG, Ellaway R. 2009.
Towards a typology of virtual patients. Med Teach 31:743–748.
Ilgen DR, Fisher CD, Taylor MS. 1979. Consequences of individual
feedback on behavior in organizations. J Appl Psychol 64:349–371.
Issenberg SB, McGaghie WC, Petrusa ER, Gordon DL, Scalese RJ. 2005.
Features and uses of high-fidelity medical simulations that lead to
effective learning: A BEME systematic review. Med Teach 27:10–28.
Jacoby J, Mazursky D, Troutman T, Kuss A. 1984. When feedback is
ignored: Disutility of outcome feedback. J Appl Psychol 69:531–545.
Johnson DS, Perlow R, Pieper KF. 1993. Differences in task performance as
a function of type of feedback: Learning-oriented versus performanceoriented feedback. J Appl Social Psychol 23:303–320.
Kaufmann C, Liu A. 2001. Trauma training: Virtual reality applications. Stud
Health Technol Inform 81:236–241.
Keane DR, Norman GR, Vickers J. 1991. The inadequacy of recent research
on computer-assisted instruction. Acad Med 66:444–448.
Kern DE, Thomas PA, Hughes MT, eds. 2009. Curriculum development for
medical education: A six-step approach, 2nd. Baltimore, MD: Johns
Hopkins University Press.
Kneebone R, Kidd J, Nestel D, Asvall S, Paraskeva P, Darzi A. 2002. An
innovative model for teaching and learning clinical procedures. Med
Educ 36:628–634.
Kobayashi L, Suner S, Shapiro MJ, Jay G, Sullivan F, Overly F, Seekell C,
Hill A, Williams KA, EMT-P NR. 2006. Multipatient disaster scenario
design using mixed modality medical simulation for the evaluation
of civilian prehospital medical response: A ‘‘dirty bomb’’ case study.
Simul Healthc 1:72–78.
Kolb DA. 1984. Experiential learning: Experience as the source of learning
and development. Englewood Cliffs, NJ: Prentice-Hall.
Lampotang S. 2003. Virtual anesthesia machine has worldwide impact.
APSF Newslett 18:57.
Lareau SA, Kyzer BD, Hawkins SC, McGinnis HD. 2010. Advanced
wilderness life support education using high-technology patient
simulation. Wilderness Environ Med 21:166–170..
Lee SK, Pardo M, Gaba D, Sowb Y, Dicker R, Straus EM, Khaw L, Morabito D,
Krummel TM, Knudson MM. 2003. Trauma assessment training with a
patient simulator: A prospective, randomized study. J Trauma 55:
651–657.
LeFlore JL,
Anderson
M.
2009. Alternative
educational
models
for
interdisciplinary student teams. Simul Healthc 4:135–142.
Locke EA, Saari LM, Shaw KN, Latham GP. 1981. Goal setting and
performance: 1969–1980. Psychol Bull 90:125–152.
Maran NJ, Glavin RJ. 2003. Low- to high-fidelity simulation – A continuum
of medical education? Med Educ 37:22–28.
McGuire CH. 1999. Simulation: Its essential nature and characteristics.
In: Tekian A, McGuire CH, McGaghie WC, editors. Innovative
simulations for assessing professional competence: From paperand-pencil to virtual reality. Chicago: University of Illinois at Chicago.
pp 3–6.
Monti
EJ,
Wren
K,
Haas
R,
Lupien
AE.
1998.
The
use
of
an
anesthesia
simulator
in
graduate
and
undergraduate
education.
CRNA 9:59–66.
Moorthy K, Smith S, Brown T, Bann S, Darzi A. 2003. Evaluation of virtual
reality bronchoscopy as a learning and assessment tool. Respiration
70:195–199.
Morris J. 2001. Peer assessment: A missing link between teaching and
learning? A review of the literature. Nurse Educ Today 21:507–515.
Norman G. 2005. Research in clinical reasoning: Past history and current
trends. Med Educ 39:418–427.
Patrick J, James N. 2004. A task-oriented perspective of situation awareness.
In: Banbury SP, Tremblay S, editors. A cognitive approach to situation
awareness: Theory and application. Hampshire, England: Ashgate
Publishing. pp 61–81.
Phillips J, Berge ZL. 2009. Second life for dental education. J Dent Educ
73:1260–1264.
Raemer DB. 2004. Team-oriented medical simulation. In: Dunn WF, editor.
Simulators in critical care education and beyond. Des Plaines, IL:
Society of Critical Care Medicine. pp 42–46.
Reader T, Flin R, Lauche K, Cuthbertson BH. 2006. Non-technical skills in
the intensive care unit. Br J Anaesth 96:551–559.
Rehmann AJ, Mitman RD, Reynolds MC. 1995. A handbook of flight
simulation fidelity requirements for human factors research. Atlantic
City,
NJ:
U.S.
Department
of
Transportation,
Federal
Aviation
Administration.
Reiser RA, Gagne´ RM. 1983. Selecting media for instruction. Englewood
Cliffs, NJ: Educational Technology Publications.
Rudolph JW, Simon R, Dufresne RL, Raemer DB. 2006. There’s no such
thing as ‘‘nonjudgmental’’ debriefing: A theory and method for
debriefing with good judgment. Simul Healthc 1:49–55.
Salas E, Bowers CA, Rhodenizer L. 1998. It is not how much you have but
how you use it: Toward a rational use of simulation to support aviation
training. Int J Aviat Psychol 8:197–208.
Salas E, Burke CS. 2002. Simulation for training is effective when. . .. Qual
Saf Health Care 11:119–120.
Salas E, Klein C, King H, Salisbury M, Augenstein JS, Birnbach DJ,
Robinson DW, Upshaw C. 2008. Debriefing medical teams: 12
G. Chiniara et al.
e1394

## Page 17

Evidence-based best practices and tips. Jt Comm J Qual Patient Saf
34:518–527.
Salas E, Wilson KA, Burke CS, Priest HA. 2005. Using simulation-based
training to improve patient safety: What does it take? Jt Comm J Qual
Patient Saf 31:363–371.
Scha¨r
SG,
Schluep
S,
Schierz
C,
Krueger
H.
2000.
Interaction
for
computer-aided
learning.
Interact
Multimedia
Electr
J
Comput-Enhanced Learn 2. [Accessed 1 February 2012] Available
from http://imej.wfu.edu/articles/2000/1/03/
Schiavenato M. 2009. Reevaluating simulation in nursing education:
Beyond the human patient simulator. J Nurs Educ 48:388–394.
Schuwirth L. 2002. Can clinical reasoning be taught or can it only be
learned? Med Educ 36:695–696.
Smith JR, Cole FS. 2009. Patient safety: Effective interdisciplinary teamwork
through simulation and debriefing in the neonatal ICU. Crit Care Nurs
Clin North Am 21:163–179.
Swing
SR.
2007.
The
ACGME
outcome
project:
Retrospective
and
prospective. Med Teach 29:648–654.
Triola M, Feldman H, Kalet AL, Zabar S, Kachur EK, Gillespie C, Anderson M,
Griesser C, Lipkin M. 2006. A randomized trial of teaching clinical skills
using virtual and live standardized patients. J Gen Intern Med 21:
424–429.
Tsai T-C, Harasym PH, Nijssen-Jordan C, Jennett P. 2006. Learning gains
derived from a high-fidelity mannequin-based simulation in the
pediatric emergency department. J Formos Med Assoc 105:94–98.
Vukanovic-Criley JM, Boker JR, Criley SR, Rajagopalan S, Criley JM. 2008.
Using virtual patients to improve cardiac examination competency in
medical students. Clin Cardiol 31:334–339.
Wallin CJ, Meurling L, Hedman L, Hedegard J, Fellander-Tsai L. 2007.
Target-focused medical emergency team training using a human patient
simulator: Effects on behaviour and attitude. Med Educ 41:173–180.
Wayne DB, Didwania A, Feinglass J, Fudala MJ, Barsuk JH, McGaghie WC.
2008. Simulation-based education improves quality of care during
cardiac arrest team responses at an academic teaching hospital: A casecontrol study. Chest 133:56–61.
Williams V. 2003. Designing simulations for learning. E-J Instruct Sci
Technol 6. [Accessed 11 November 2011] Available from http://
www.ascilite.org.au/ajet/e-jist/docs/Vol6_No1/contents2.htm
Youngblood P, Harter PM, Srivastava S, Moffett S, Heinrichs WL, Dev P.
2008. Design, development, and evaluation of an online virtual
emergency department for training trauma teams. Simul Healthc
3:146–153.
Youngblood P, Heinrichs L, Cornelius C, Dev P. 2007. Designing casebased learning for virtual worlds. Simul Healthc 2:246–247.
A framework for simulation in healthcare
e1395
